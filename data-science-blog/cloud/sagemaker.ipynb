{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8399f7e-04b5-4cfb-908c-943d3d22432b",
   "metadata": {},
   "source": [
    "# üßò AWS Sagemaker"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f1cb2ae-f294-46fa-b8bc-16156b03d626",
   "metadata": {},
   "source": [
    "## ‚òÅÔ∏è AWS Sagemaker Serverless\n",
    "\n",
    "Major cost savings on ML endpoints with AWS Sagemaker!\n",
    "\n",
    "- In the past when you deploy an ML model as a Real-Time Inference endpoint on Sagemaker, you‚Äôve had to keep the deployment instance running 24*7\n",
    "- This is a waste of money if your ML model receives sporadic traffic with some periods of 0 traffic as your instance is running idle\n",
    "\n",
    "Enter Sagemaker serverless!\n",
    "\n",
    "- During times when there is no traffic, Serverless Inference scales your endpoint down to 0, helping you to minimize your costs!\n",
    "- There will be some cold start when there is traffic after a long period of inactivity. In my experience with scikit-learn models, the cold start is <2s!\n",
    "- Implement with just one line of code change\n",
    "\n",
    "üìñ Docs: https://docs.aws.amazon.com/sagemaker/latest/dg/serverless-endpoints.html\n",
    "\n",
    "#datascience #dataanalytics #dataengineering #aws #gcp #azure #python\n",
    "\n",
    "```{image} images/sagemaker/Repos-sagemaker_serverless.png\n",
    ":alt: sagemaker serverless\n",
    ":class: bg-primary mb-1\n",
    ":width: 100%\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75db3831-7677-495b-8d04-aee74e6110d5",
   "metadata": {},
   "source": [
    "## üöÄ AWS Sagemaker Inference\n",
    "\n",
    "AWS Sagemaker is one of the easiest ways I‚Äôve come across for deploying Machine Learning models as a Data Scientist.\n",
    "\n",
    "- Create a simple `serve.py` file with just four functions\n",
    "- model_fn: To load the model from a file\n",
    "- input_fn: Convert input JSON data into a pandas dataframe. Do any other preprocessing BEFORE model predictions here\n",
    "- predict_fn: Run the model predictions\n",
    "- output_fn: Post-process the predictions and convert them to JSON\n",
    "- As a pre-requisite, your trained model should be zipped as a tar.gz file on S3\n",
    "\n",
    "Use the same `http://serve.py` for all of the below deployment methods!\n",
    "\n",
    "- Real-time inference\n",
    "- Multi-model endpoint\n",
    "- Serverless inference (the topic of yesterday‚Äôs post)\n",
    "- Batch transform\n",
    "\n",
    "The entire process is massively simplified if you use the JSON lines format (a topic for another post).\n",
    "\n",
    "üìñ Docs: https://sagemaker.readthedocs.io/en/stable/frameworks/sklearn/using_sklearn.html\n",
    "\n",
    "#mlops #datascience #dataanalytics #dataengineering #aws #sagemaker #gcp #azure #python\n",
    "\n",
    "```{image} images/sagemaker/Repos-sagemaker_serve_py.png\n",
    ":alt: sagemaker serve.py\n",
    ":class: bg-primary mb-1\n",
    ":width: 100%\n",
    ":align: center\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0036960e-cdea-440f-a2bb-2cce46b5f87e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
