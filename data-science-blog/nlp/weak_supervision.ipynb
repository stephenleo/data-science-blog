{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3861373c-69fc-4751-9047-c5d71a5cd842",
   "metadata": {},
   "source": [
    "# 🗂️ Weak Supervision\n",
    "\n",
    "- Weak Supervision is the Data Centric AI technique to \n",
    "> Reduce the efforts of manual labeling while unlocking the vast knowledge of domain subject matter experts (SMEs) by leveraging a diversity of weaker, often programmatic supervision sources.\n",
    "\n",
    "![](images/weak_supervision/drake_meme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0d3dad-63eb-4411-9b0a-2e4d0b34bc73",
   "metadata": {},
   "source": [
    "## 🎤 No Training Data? No Problem! Weak Supervision to the Rescue! \n",
    "\n",
    "- Presentation at Quantum Black Meetup at Singapore on May 19, 2022: [Link to slides](https://www.slideshare.net/StephenLeo7/weak-supervisionpdf)\n",
    "- The topics I covered are,\n",
    "    - ML's insatiable need for large datasets\n",
    "    - Contemporary ML leaving out domain knowledge from Subject Matter Experts\n",
    "    - How Weak Supervision, an approach of Data-Centric AI, solves both the problems simultaneously by encoding domain subject matter expertise into programmatic labeling functions.\n",
    "    - The WRENCH benchmark to compare various weak supervision algorithms on several standard datasets.\n",
    "    - Snorkel to combine the various labeling functions.\n",
    "    - COSINE to fine-tune a final transformer based model that overcomes the noise in weak labels\n",
    "    - Future Directions and Resources\n",
    "\n",
    "- The slides have several links to all the necessary resources if you're interested to read more.\n",
    "\n",
    "- Feel free to use the slides but please remember to credit me with a link back to this repository or my Linkedin profile!\n",
    "\n",
    "#datascience #machinelearning #artificialintelligence #nlp #algorithms\n",
    "\n",
    "[![](images/weak_supervision/quantum_black_meetup_talk.png)](https://www.slideshare.net/StephenLeo7/weak-supervisionpdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a6dd35-16bd-4a7c-b4f2-d5c5b16f0dae",
   "metadata": {},
   "source": [
    "## 🦦 WeaSEL: Weakly Supervised End-to-end Learning\n",
    "\n",
    "An interesting paper/code for those of us working on problems that have low/no labels but high SME domain knowledge. **WeaSEL: Weakly Supervised End-to-end Learning**.\n",
    "\n",
    "Train your favorite neural network for weakly-supervised classification:\n",
    "\n",
    "✅ With only labeling functions (LFs), i.e. without any labeled training data!\n",
    "\n",
    "✅ In an end-to-end manner, i.e. directly train and evaluate your neural net, there's no need to train a separate label model anymore as in Snorkel\n",
    "\n",
    "✅ With a better test set performance and enhanced robustness against correlated or inaccurate LFs than prior methods like Snorkel\n",
    "\n",
    "🌟 Github: [https://github.com/autonlab/weasel](https://github.com/autonlab/weasel)\n",
    "\n",
    "📖 Paper: [https://arxiv.org/abs/2107.02233](https://arxiv.org/abs/2107.02233)\n",
    "\n",
    "Have you used any other weak supervision library? Please share in the comments!\n",
    "\n",
    "#nlp #machinelearning #datascience #researchpaper #github #dataprogramming #weaksupervision\n",
    "\n",
    "![](images/weak_supervision/weasel_performance.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfdbbbb-a4b5-4c9f-a996-41aa6426bff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
